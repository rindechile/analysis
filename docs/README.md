# Mercado P√∫blico PDF Scraper

Automated scraper to download PDF documents from Chilean public procurement website (Mercado P√∫blico) for all purchases in `data/purchases.csv`.

## Features

‚úÖ **Incremental scraping** - Only scrapes NEW codes when data is updated weekly  
‚úÖ **Permanent registry** - `scraped-codes.json` tracks all-time scraped codes  
‚úÖ **Resumable sessions** - Checkpoint system allows resuming interrupted scrapes  
‚úÖ **Rate-limited** - Randomized delays (1-3s) between requests to avoid detection  
‚úÖ **Anti-detection** - Headed browser mode with realistic user agent and headers  
‚úÖ **Error handling** - Automatic retry with exponential backoff (up to 3 attempts)  
‚úÖ **Progress tracking** - Real-time progress logging and final summary report  
‚úÖ **Directory structure** - PDFs organized by `chilecompra_code` in separate folders

## Directory Structure

```
docs/
‚îú‚îÄ‚îÄ scrape-documents.ts       # Main scraper script
‚îú‚îÄ‚îÄ scraper-utils.ts          # Utility functions
‚îú‚îÄ‚îÄ csv-utils.ts              # CSV parsing utilities
‚îú‚îÄ‚îÄ scraped-codes.json        # PERMANENT registry of all scraped codes (all-time)
‚îú‚îÄ‚îÄ checkpoint.json           # Session-based progress tracker
‚îú‚îÄ‚îÄ download-manifest.json    # JSON manifest of successful downloads
‚îî‚îÄ‚îÄ downloads/                # PDF output directory
    ‚îî‚îÄ‚îÄ {chilecompra_code}/   # One folder per purchase code
        ‚îî‚îÄ‚îÄ *.pdf             # Downloaded PDFs with original names

data/
‚îî‚îÄ‚îÄ purchases.csv             # Source data with purchase codes
```

## Weekly Update Workflow

When new data arrives weekly:

1. **Update CSV**: Replace or append to `data/purchases.csv` with new purchase codes
2. **Run scraper**: `pnpm run scrape` - automatically detects and scrapes only NEW codes
3. **Review**: Check the summary to see how many new codes were processed

The scraper automatically:
- Loads `scraped-codes.json` to get all previously scraped codes
- Compares against the CSV to find NEW codes only
- Scrapes only the new codes
- Updates the registry with newly scraped codes

## Usage

### Basic Usage (Resume from checkpoint)
```bash
pnpm tsx docs/scrape-documents.ts
# or
pnpm run scrape
```

### Start Fresh (Ignore checkpoint)
```bash
pnpm tsx docs/scrape-documents.ts --fresh
# or
pnpm run scrape:fresh
```

### Test Mode (First N codes only)
```bash
pnpm tsx docs/scrape-documents.ts --test=10
# or
pnpm run scrape:test
```

### Retry Failed Codes
```bash
pnpm tsx docs/scrape-documents.ts --retry
# or
pnpm run scrape:retry
```

## How It Works

The scraper follows a 3-stage navigation flow for each purchase code:

### Stage 1: Search & Navigate
- Goes to `https://buscador.mercadopublico.cl/ordenes-de-compra?keywords={code}`
- Finds the link matching the purchase code
- Clicks the link (opens a new page with purchase order details)

### Stage 2: Open Attachments
- On the detail page, locates the attachments button (`#imgAttachments`)
- Clicks the button to open the attachments popup window

### Stage 3: Download PDFs
- In the popup, finds all "Ver" (view) buttons for PDFs
- Clicks each button to trigger PDF download
- Saves files with original names to `downloads/{code}/`

## Important Notes

‚ö†Ô∏è **Browser Mode**: The scraper runs in **headed mode** (browser window visible) because CloudFront blocks headless browsers. Do not change `headless: false`.

‚ö†Ô∏è **Rate Limiting**: Random delays are built-in. Do not remove them or you may trigger rate limiting/blocking.

‚ö†Ô∏è **Long Running**: With ~48,000 codes, expect this to take several days. Use checkpoint system to resume.

‚ö†Ô∏è **No Attachments**: Some purchases have no documents - this is logged as success with 0 PDFs.

## Checkpoint System

The checkpoint file (`checkpoint.json`) tracks:
- ‚úÖ Successfully processed codes
- ‚ùå Failed codes with error messages and attempt counts
- üìä Statistics (total processed, total failed)
- ‚è±Ô∏è Last processed timestamp

To reset and start over, delete `checkpoint.json`.

## Output

### Download Manifest
`download-manifest.json` contains:
```json
[
  {
    "code": "3707-351-AG25",
    "success": true,
    "pdfCount": 2,
    "directory": "/path/to/downloads/3707-351-AG25"
  }
]
```

### Progress Logging
```
üìä Progress: 100/48304 (0.2%) | ‚úÖ 95 | ‚ùå 5
‚úÖ [1/48304] 3707-351-AG25: 2 PDFs
‚ùå [2/48304] 1234-567-AG25: Failed to navigate to purchase order
‚è≠Ô∏è  Already processed: 3707-351-AG25
```

## Troubleshooting

### CloudFront 403 Error
If you see "The request could not be satisfied" errors:
- Ensure `headless: false` in the browser launch config
- Check that user agent and headers are properly set
- Increase random delays between requests

### Timeout Errors
- Increase `PAGE_TIMEOUT` constant (default: 60s)
- Check internet connection stability
- Some pages may genuinely be slow - retry logic should handle this

### No PDFs Downloaded
- Some purchases legitimately have no attachments
- Check `download-manifest.json` to see pdfCount
- Manually verify the purchase code on Mercado P√∫blico website

## Data Source

Reads from: `/schemas/data/purchase.csv`

Expected columns:
- Column 1: `chilecompra_code` (format: `XXXXXX-XXX-XX25`)
- Other columns: municipality_id, supplier_rut, prices, etc.

## Dependencies

- `playwright` - Browser automation
- `p-limit` - Concurrency control
- Node.js built-ins - fs, path

## Configuration

Edit these constants in `scrape-documents.ts` to adjust behavior:

```typescript
const PAGE_TIMEOUT = 60000;      // Page load timeout (ms)
const MAX_RETRIES = 3;           // Retry attempts per code
const RETRY_BASE_DELAY = 2000;   // Base delay for exponential backoff (ms)
const CONCURRENCY = 1;           // Number of parallel browsers (recommend 1)
```

## Performance

- **Speed**: ~3-5 seconds per code (with delays)
- **Total Time**: 48,304 codes √ó 4s avg ‚âà 54 hours
- **Storage**: Varies by PDF size, estimate 5-10GB total

## License

Same as parent project.
